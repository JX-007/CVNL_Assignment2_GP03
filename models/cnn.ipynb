{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os \n",
    "from pathlib import Path \n",
    "from typing import Any, Callable \n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import download_and_extract_archive, verify_str_arg\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from sklearn.metrics import *\n",
    "import torch.nn as nn\n",
    "\n",
    "class FGVCAircraft(VisionDataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    - ``variant``, e.g. Boeing 737-700. A variant collapses all the models that are visually\n",
    "        indistinguishable into one class. The dataset comprises 100 different variants.\n",
    "    - ``family``, e.g. Boeing 737. The dataset comprises 70 different families.\n",
    "    - ``manufacturer``, e.g. Boeing. The dataset comprises 30 different manufacturers.\n",
    "    \"\"\"\n",
    "\n",
    "    _URL = \"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str | Path,\n",
    "        split: str = \"trainval\",\n",
    "        annotation_level: str = \"variant\",\n",
    "        transform: Callable | None = None,\n",
    "        target_transform: Callable | None = None,\n",
    "        download: bool = False,\n",
    "        loader: Callable[[str], Any] = default_loader,\n",
    "    ) -> None:\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self._split = verify_str_arg(split, \"split\", (\"train\", \"val\", \"trainval\", \"test\"))\n",
    "        self._annotation_level = verify_str_arg(\n",
    "            annotation_level, \"annotation_level\", (\"variant\", \"family\", \"manufacturer\")\n",
    "        )\n",
    "\n",
    "        self._data_path = os.path.join(self.root, \"fgvc-aircraft-2013b\")\n",
    "        if download:\n",
    "            self._download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\"Dataset not found. download=True\")\n",
    "\n",
    "        annotation_file = os.path.join(\n",
    "            self._data_path,\n",
    "            \"data\",\n",
    "            {\n",
    "                \"variant\": \"variants.txt\",\n",
    "                \"family\": \"families.txt\",\n",
    "                \"manufacturer\": \"manufacturers.txt\",\n",
    "            }[self._annotation_level],\n",
    "        )\n",
    "        with open(annotation_file) as f:\n",
    "            self.classes = [line.strip() for line in f]\n",
    "\n",
    "        self.class_to_idx = dict(zip(self.classes, range(len(self.classes))))\n",
    "\n",
    "        image_data_folder = os.path.join(self._data_path, \"data\", \"images\")\n",
    "        labels_file = os.path.join(self._data_path, \"data\", f\"images_{self._annotation_level}_{self._split}.txt\")\n",
    "\n",
    "        self._image_files = []\n",
    "        self._labels = []\n",
    "\n",
    "        with open(labels_file) as f:\n",
    "            for line in f:\n",
    "                image_name, label_name = line.strip().split(\" \", 1)\n",
    "                self._image_files.append(os.path.join(image_data_folder, f\"{image_name}.jpg\"))\n",
    "                self._labels.append(self.class_to_idx[label_name])\n",
    "        self.loader = loader\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._image_files)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Any, Any]:\n",
    "        image_file, label = self._image_files[idx], self._labels[idx]\n",
    "        image = self.loader(image_file)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "    def _download(self) -> None:\n",
    "        \"\"\"\n",
    "        Download the FGVC Aircraft dataset archive and extract it under root.\n",
    "        \"\"\"\n",
    "        if self._check_exists():\n",
    "            return\n",
    "        download_and_extract_archive(self._URL, self.root)\n",
    "\n",
    "    def _check_exists(self) -> bool:\n",
    "        return os.path.exists(self._data_path) and os.path.isdir(self._data_path)\n",
    "    \n",
    "        \n",
    "class RemoveCopyrightBanner:\n",
    "    def __init__(self, banner_height=20):\n",
    "        self.banner_height = banner_height\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        img: PIL Image\n",
    "        Returns cropped PIL Image with bottom banner removed\n",
    "        \"\"\"\n",
    "        width, height = img.size\n",
    "        return img.crop((0, 0, width, height - self.banner_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.ipynb\n",
    "\n",
    "# libraries used for CNN Classifier Model\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Transform Datasets for Training and Testing Use\n",
    "train_transform = transforms.Compose([\n",
    "    RemoveCopyrightBanner(20),\n",
    "    transforms.RandomResizedCrop(\n",
    "        size=224,\n",
    "        scale=(0.8, 1.0)   # zoom in/out slightly\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),      \n",
    "    transforms.ColorJitter(             # simulate colour effects of weather/ bad lighting\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    RemoveCopyrightBanner(banner_height=20),   \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Create the dataset, automatically download if missing\n",
    "train_dataset = FGVCAircraft(\n",
    "    root=\"/content\",             # Colab working folder\n",
    "    split=\"train\",               # train/val/test\n",
    "    annotation_level=\"variant\",  # variant/family/manufacturer\n",
    "    transform=train_transform,\n",
    "    download=True                # triggers download if not found\n",
    ")\n",
    "\n",
    "val_dataset = FGVCAircraft(      # test dataset\n",
    "    root=\"/content\",\n",
    "    split=\"val\",\n",
    "    annotation_level=\"variant\",\n",
    "    transform=val_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Access length\n",
    "print(f\"{len(train_dataset)} examples for training and {len(val_dataset)} for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed288bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify transformation\n",
    "# Get one sample\n",
    "img, label = train_dataset[0]\n",
    "class_name = train_dataset.classes[label]\n",
    "# Unnormalize for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "img = img * std + mean\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "plt.title(f\"Variant: {class_name}\")\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd90ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data in evaluation dataset\n",
    "img, label = val_dataset[0]\n",
    "class_name = val_dataset.classes[label]\n",
    "\n",
    "# Unnormalize for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "img = img * std + mean\n",
    "\n",
    "# Convert from CHW → HWC\n",
    "plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "plt.title(f\"Variant: {class_name}\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(3, 64),    # 224 -> 112\n",
    "            conv_block(64, 128),  # 112 -> 56\n",
    "            conv_block(128, 256), # 56 -> 28\n",
    "            conv_block(256, 512), # 28 -> 14\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(num_classes)\n",
    "\n",
    "# Load pretrained resnet18\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the last fully connected layer\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "# Optional: freeze all layers except last FC\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "resnet18.fc.requires_grad = True\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-4)\n",
    "\n",
    "# define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(\n",
    "  train_dataset, \n",
    "  batch_size=BATCH_SIZE, \n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  val_dataset, \n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# initialize model, loss & optimizer\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# function to train model for 1 epoch\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  for images, labels in loader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_loss += loss.item()\n",
    "      preds = outputs.argmax(dim=1)\n",
    "      correct += (preds == labels).sum().item()\n",
    "      total += labels.size(0)\n",
    "\n",
    "  return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844de9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 15 (2452869365.py, line 16)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbest_val_loss = val_loss\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 15\n"
     ]
    }
   ],
   "source": [
    "# Run training loop\n",
    "model = DeepCNN(num_classes).to(device)\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_cnn.pth\")\n",
    "        print(f\"Saved new best model at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4209f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Layer\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_predictions(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = get_predictions(model, val_loader)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=train_dataset.classes\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=train_dataset.classes,\n",
    "    yticklabels=train_dataset.classes,\n",
    "    square=True,\n",
    "    cbar=True\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix – Aircraft Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find misclassified images\n",
    "misclassified_idx = np.where(y_true != y_pred)[0]\n",
    "print(\"Total misclassified images:\", len(misclassified_idx))\n",
    "\n",
    "# visual representation of misclassified images\n",
    "def show_misclassified(dataset, y_true, y_pred, indices, n=5):\n",
    "    samples = random.sample(list(indices), n)\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, idx in enumerate(samples):\n",
    "        img, _ = dataset[idx]\n",
    "\n",
    "        # unnormalize\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "        img = img * std + mean\n",
    "\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(img.permute(1,2,0).clamp(0,1))\n",
    "        plt.title(\n",
    "            f\"True: {train_dataset.classes[y_true[idx]]}\\n\"\n",
    "            f\"Pred: {train_dataset.classes[y_pred[idx]]}\"\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Model\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load saved model\n",
    "model = torch.load(\"best_cnn.pth\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Path to your real-world aircraft image\n",
    "img_path = \"test_aircraft.jpg\" # replace w any img from evaluation dataset\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Transform (same as val_transform)\n",
    "transform = transforms.Compose([\n",
    "    RemoveCopyrightBanner(20),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img_tensor = transform(img).unsqueeze(0)  # add batch dimension\n",
    "img_tensor = img_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)\n",
    "    predicted_class = outputs.argmax(dim=1).item()\n",
    "    \n",
    "# Map index to class name\n",
    "idx_to_class = {i: c for i, c in enumerate(train_dataset.classes)}\n",
    "print(\"Predicted Aircraft Variant:\", idx_to_class[predicted_class])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3242ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unnormalize for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "\n",
    "img_show = img_tensor.cpu()[0] * std + mean\n",
    "plt.imshow(img_show.permute(1,2,0).clamp(0,1))\n",
    "plt.title(f\"Predicted: {idx_to_class[predicted_class]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
